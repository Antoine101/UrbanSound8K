{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb211a39",
   "metadata": {},
   "source": [
    "# <center>Study of UrbanSound8K Classes Acoustics Signature</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d04f31",
   "metadata": {},
   "source": [
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import IPython\n",
    "import datetime\n",
    "import librosa.display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c2313",
   "metadata": {},
   "source": [
    "## Import of the Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc142b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"dataset/UrbanSound8K.csv\")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45772d24",
   "metadata": {},
   "source": [
    "## Torchaudio Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5939c7",
   "metadata": {},
   "source": [
    "### Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_transform = torchaudio.transforms.Spectrogram( \n",
    "                    # Size of FFT, creates n_fft // 2 + 1 bins. (Default: 400)\n",
    "                    n_fft = 256,\n",
    "                    # Window size. (Default: n_fft)\n",
    "                    win_length = 256,\n",
    "                    # Length of hop between STFT windows. (Default: win_length // 2)\n",
    "                    hop_length = 128,\n",
    "                    # Two sided padding of signal. (Default: 0)\n",
    "                    pad = 0,\n",
    "                    # A function to create a window tensor that is applied/multiplied to each frame/window. (Default: torch.hann_window)\n",
    "                    window_fn = torch.hann_window,\n",
    "                    # Exponent for the magnitude spectrogram, (must be > 0) e.g., 1 for energy, 2 for power, etc. (Default: 2)\n",
    "                    power = 2,\n",
    "                    # Whether to normalize by magnitude after stft. (Default: False)\n",
    "                    normalized = True,\n",
    "                    # Arguments for window function. (Default: None)\n",
    "                    wkwargs = None,\n",
    "                    # Whether to pad waveform on both sides so that the t-th frame is centered at time t x hop_length (Default: True)\n",
    "                    center = False,\n",
    "                    # Controls the padding method used when center is True. (Default: \"reflect\")\n",
    "                    pad_mode = \"reflect\",\n",
    "                    # Controls whether to return half of results to avoid redundancy. (Default: True)\n",
    "                    onesided = True,\n",
    "                    # Indicates whether the resulting complex-valued Tensor should be represented with native complex dtype, \n",
    "                    # such as torch.cfloat and torch.cdouble, or real dtype mimicking complex value with an extra dimension \n",
    "                    # for real and imaginary parts. (See also torch.view_as_real.)\n",
    "                    # This argument is only effective when power=None. It is ignored for cases where power is a number as in those cases, the returned tensor is power spectrogram, which is a real-valued tensor.\n",
    "                    return_complex = False\n",
    "                    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d515013",
   "metadata": {},
   "source": [
    "### Mel-Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65fdc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
    "                    # Sample rate of audio signal. (Default: 16000)\n",
    "                    sample_rate = 22050,\n",
    "                    # Size of FFT, creates n_fft // 2 + 1 bins. (Default: 400)\n",
    "                    n_fft = 256,\n",
    "                    # Window size. (Default: n_fft)\n",
    "                    win_length = 256,\n",
    "                    # Length of hop between STFT windows. (Default: win_length // 2)\n",
    "                    hop_length = 128,\n",
    "                    # Minimum frequency. (Default: 0.)\n",
    "                    f_min = 0.,\n",
    "                    # Maximum frequency. (Default: None)\n",
    "                    f_max = 20000,\n",
    "                    # Two sided padding of signal. (Default: 0)    \n",
    "                    pad = 0,\n",
    "                    # Number of mel filterbanks. (Default: 128)\n",
    "                    n_mels = 128,\n",
    "                    # A function to create a window tensor that is applied/multiplied to each frame/window. (Default: torch.hann_window)\n",
    "                    window_fn = torch.hann_window,\n",
    "                    # Exponent for the magnitude spectrogram, (must be > 0) e.g., 1 for energy, 2 for power, etc. (Default: 2)\n",
    "                    power = 2,\n",
    "                    # Whether to normalize by magnitude after stft. (Default: False)\n",
    "                    normalized = True,\n",
    "                    # Arguments for window function. (Default: None)\n",
    "                    wkwargs = None, \n",
    "                    # Whether to pad waveform on both sides so that the t-th frame is centered at time t x hop_length (Default: True)\n",
    "                    center = True, \n",
    "                    # Controls the padding method used when center is True. (Default: \"reflect\")\n",
    "                    pad_mode = \"reflect\", \n",
    "                    # Controls whether to return half of results to avoid redundancy. (Default: True)\n",
    "                    onesided = True, \n",
    "                    # If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). (Default: None)\n",
    "                    norm = None,\n",
    "                    # Scale to use: htk or slaney. (Default: \"htk\")\n",
    "                    mel_scale = \"htk\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f6300",
   "metadata": {},
   "source": [
    "### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2843f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "                    # Sample rate of audio signal. (Default: 16000)\n",
    "                    sample_rate = 22050,\n",
    "                    # Number of mfc coefficients to retain. (Default: 40)\n",
    "                    n_mfcc = 40,\n",
    "                    # Type of DCT (discrete cosine transform) to use. (Default: 2)\n",
    "                    dct_type = 2,\n",
    "                    # Norm to use. (Default: \"ortho\")\n",
    "                    norm = \"ortho\", \n",
    "                    # Whether to use log-mel spectrograms instead of db-scaled. (Default: False)\n",
    "                    log_mels = True,\n",
    "                    # Arguments for MelSpectrogram (Default: None)\n",
    "                    melkwargs = None\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb412c",
   "metadata": {},
   "source": [
    "### Amplitude to dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03338ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_to_db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "                    # Scale of input tensor (\"power\" or \"magnitude\"). The power being the elementwise square of the magnitude. (Default: \"power\")\n",
    "                    stype = \"power\",\n",
    "                    # Minimum negative cut-off in decibels. A reasonable number is 80. (Default: None)\n",
    "                    top_db = None\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae879ec9",
   "metadata": {},
   "source": [
    "## Audio Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = 4\n",
    "target_sample_rate = 22050\n",
    "n_samples = target_length * target_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d8b34",
   "metadata": {},
   "source": [
    "## Study of a Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79243937",
   "metadata": {},
   "source": [
    "For a recall, the different classes present in the UrbanSound8K dataset are:\n",
    "- air_conditioner\n",
    "- car_horn\n",
    "- dog_bark\n",
    "- drilling\n",
    "- engine_idling\n",
    "- gun_shot\n",
    "- jackhammer\n",
    "- siren\n",
    "- street_music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f346ba",
   "metadata": {},
   "source": [
    "### Selection of Multiple Audios From One Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input annotations filtering critera\n",
    "label = \"dog_bark\"\n",
    "salience = 1 \n",
    "n_audios = 10\n",
    "\n",
    "# Filter annotations based on criteria\n",
    "filtered_metadata = metadata.loc[\n",
    "    (metadata[\"class\"]==label)\n",
    "    & (metadata[\"salience\"]==salience)\n",
    "    ]\n",
    "\n",
    "# Randomly select an audio from the filtered annotations\n",
    "selected_audios = filtered_metadata.sample(n=n_audios) \n",
    "display(selected_audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a dictionnary to store each of the selected audio paths and their corresponding pre-processed temporal signal\n",
    "audios_signal = {}\n",
    "\n",
    "for index, row in selected_audios.iterrows():\n",
    "    audio_filename = row[\"slice_file_name\"]\n",
    "    audio_fold = f\"fold{row['fold']}\"\n",
    "    audio_path = os.path.join(\"dataset\", audio_fold, audio_filename)\n",
    "    ### Load the audio and pre-process it\n",
    "    # Load the audio signal\n",
    "    audio_signal, sr = torchaudio.load(audio_path)\n",
    "    # Mix it down to mono if necessary\n",
    "    if audio_signal.shape[0] > 1:\n",
    "        audio_signal = torch.mean(audio_signal, dim=0, keepdim=True)\n",
    "    # Resample it\n",
    "    resample_transform = transforms.Resample(sr, target_sample_rate)\n",
    "    audio_signal = resample_transform(audio_signal)\n",
    "    # Cut if necessary\n",
    "    if audio_signal.shape[1] > n_samples:\n",
    "        audio_signal = event_signal[:, :n_samples]\n",
    "    # Right pad if necessary\n",
    "    if audio_signal.shape[1] < n_samples:\n",
    "        n_missing_samples = n_samples - audio_signal.shape[1]\n",
    "        last_dim_padding = (0, n_missing_samples)\n",
    "        audio_signal = nn.functional.pad(audio_signal, last_dim_padding)\n",
    "    audios_signal[audio_path] = audio_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eade59",
   "metadata": {},
   "source": [
    "### Temporal Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd2731",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(selected_audios),1, figsize=(15, 50))\n",
    "\n",
    "for i, (audio_path, audio_signal) in enumerate(audios_signal.items()):\n",
    "    #audio_signal = audio_signal.numpy()\n",
    "    \n",
    "    num_channels, num_frames = audio_signal.shape\n",
    "    time_axis = torch.arange(0, num_frames) / target_sample_rate\n",
    "    \n",
    "    axs[i].plot(time_axis, audio_signal[0], linewidth=1)\n",
    "    axs[i].set_ylabel(\"Amplitude\")\n",
    "    axs[i].set_xlabel(\"Time (s)\")\n",
    "    axs[i].set_xlim([0, 4])    \n",
    "    axs[i].set_ylim([-1, 1])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18175598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(selected_audios),1, figsize=(15, 50))\n",
    "\n",
    "for i, (audio_path, audio_signal) in enumerate(audios_signal.items()):\n",
    "    \n",
    "    num_channels, num_frames = audio_signal.shape\n",
    "    time_axis = torch.arange(0, num_frames) / target_sample_rate\n",
    "    \n",
    "    spectrogram = torch.squeeze(spectrogram_transform(audio_signal)[0])\n",
    "    spectrogram_db = amplitude_to_db_transform(spectrogram)\n",
    "    \n",
    "    n_fft_spec = (spectrogram_db.shape[0] - 1) * 2\n",
    "    frequency = (target_sample_rate / n_fft_spec) * np.linspace(0, n_fft_spec/2, spectrogram_db.shape[0])\n",
    "    max_frequency_bin = frequency_axis.max()\n",
    "    \n",
    "    axs[i].imshow(spectrogram_db, extent=[0, target_length, 0, target_sample_rate/2], origin=\"lower\", aspect=\"auto\")\n",
    "    axs[i].set_ylabel(\"Frequency (Hz)\")\n",
    "    axs[i].set_xlabel(\"Time (s)\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e62e00",
   "metadata": {},
   "source": [
    "### Mel-Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a11557",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(selected_audios),1, figsize=(15, 50))\n",
    "\n",
    "for i, (audio_path, audio_signal) in enumerate(audios_signal.items()):\n",
    "    \n",
    "    num_channels, num_frames = audio_signal.shape\n",
    "    time_axis = torch.arange(0, num_frames) / target_sample_rate\n",
    "    \n",
    "    mel_spectrogram = torch.squeeze(mel_spectrogram_transform(audio_signal)[0])\n",
    "    \n",
    "    axs[i].imshow(mel_spectrogram, extent=[0, target_length, 0, mel_spectrogram.shape[0]+1], origin=\"lower\", aspect=\"auto\")\n",
    "    axs[i].set_ylabel(\"Mel Bands\")\n",
    "    axs[i].set_xlabel(\"Time (s)\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb95b6",
   "metadata": {},
   "source": [
    "### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9434a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(selected_audios),1, figsize=(15, 50))\n",
    "\n",
    "for i, (audio_path, audio_signal) in enumerate(audios_signal.items()):\n",
    "    \n",
    "    num_channels, num_frames = audio_signal.shape\n",
    "    time_axis = torch.arange(0, num_frames) / target_sample_rate\n",
    "    \n",
    "    mfcc = torch.squeeze(mfcc_transform(audio_signal)[0])\n",
    "    \n",
    "    axs[i].imshow(mfcc, extent=[0, target_length, 0, mfcc.shape[0]+1], origin=\"lower\", aspect=\"auto\")\n",
    "    axs[i].set_ylabel(\"MFCC\")\n",
    "    axs[i].set_xlabel(\"Time (s)\")\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbcc0c",
   "metadata": {},
   "source": [
    "## Comparison of Multiple Audios From the Same Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e757c05",
   "metadata": {},
   "source": [
    "## Comparison of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fdf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = metadata[\"class\"].unique()\n",
    "selected_audios = {classe: np.nan for classe in classes}\n",
    "\n",
    "for classe in classes:\n",
    "    metadata_classe = metadata[metadata[\"class\"]==classe]\n",
    "    selected_audio = metadata_classe.sample(n=1) \n",
    "    selected_audio_filename = selected_audio.iloc[0][\"slice_file_name\"]\n",
    "    selected_audio_fold = f\"fold{selected_audio.iloc[0]['fold']}\"\n",
    "    selected_audio_path = os.path.join(\"dataset\", selected_audio_fold, selected_audio_filename)\n",
    "    selected_audios[classe] = selected_audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79763237",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,80), constrained_layout=True)\n",
    "subfigs = fig.subfigures(len(selected_audios), 1)\n",
    "\n",
    "for i, (classe, audio_path) in enumerate(selected_audios.items()):\n",
    "    ### Load the audio and pre-process it\n",
    "    # Load the audio signal\n",
    "    audio_signal, sr = torchaudio.load(audio_path)\n",
    "    # Mix it down to mono if necessary\n",
    "    if audio_signal.shape[0] > 1:\n",
    "        audio_signal = torch.mean(audio_signal, dim=0, keepdim=True)\n",
    "    # Resample it\n",
    "    resample_transform = transforms.Resample(sr, target_sample_rate)\n",
    "    audio_signal = resample_transform(audio_signal)\n",
    "    # Cut if necessary\n",
    "    if audio_signal.shape[1] > n_samples:\n",
    "        audio_signal = event_signal[:, :n_samples]\n",
    "    # Right pad if necessary\n",
    "    if audio_signal.shape[1] < n_samples:\n",
    "        n_missing_samples = n_samples - audio_signal.shape[1]\n",
    "        last_dim_padding = (0, n_missing_samples)\n",
    "        audio_signal = nn.functional.pad(audio_signal, last_dim_padding)\n",
    "        \n",
    "    ### Compute the Spectrogram\n",
    "    spectrogram = torch.squeeze(spectrogram_transform(audio_signal)[0])\n",
    "    spectrogram_db = amplitude_to_db_transform(spectrogram)\n",
    "    \n",
    "    ### Compute the Mel-Spectrogram\n",
    "    mel_spectrogram = torch.squeeze(mel_spectrogram_transform(audio_signal)[0])\n",
    "    \n",
    "    ### Compute the MFCCs\n",
    "    mfcc = torch.squeeze(mfcc_transform(audio_signal)[0])\n",
    "\n",
    "    subfig = subfigs[i]\n",
    "    subfig.suptitle(classe, fontsize=16, fontweight=\"bold\")\n",
    "    axs = subfig.subplots(1, 3)\n",
    "    axs[0].imshow(spectrogram, extent=[0, target_length, 0, target_sample_rate/2], origin=\"lower\", aspect=\"auto\")\n",
    "    axs[0].set_title(\"Spectrogram\")\n",
    "    axs[0].set_xlabel(\"Time (s)\")\n",
    "    axs[0].set_ylabel(\"Frequency (Hz)\")    \n",
    "    axs[1].imshow(mel_spectrogram, extent=[0, target_length, 0, mel_spectrogram.shape[0]+1], origin=\"lower\", aspect=\"auto\")\n",
    "    axs[1].set_title(\"Mel-Spectrogram\")\n",
    "    axs[1].set_xlabel(\"Time (s)\")\n",
    "    axs[1].set_ylabel(\"Mel Bands\")    \n",
    "    axs[2].imshow(mfcc, extent=[0, target_length, 0, mfcc.shape[0]+1], origin=\"lower\", aspect=\"auto\")\n",
    "    axs[2].set_title(\"MFCC\")\n",
    "    axs[2].set_xlabel(\"Time (s)\")\n",
    "    axs[2].set_ylabel(\"MFCC\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
