{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf916f83",
   "metadata": {},
   "source": [
    "# <center>UrbanSound8K</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd51f4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries-Import\" data-toc-modified-id=\"Libraries-Import-1\">Libraries Import</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\">Parameters</a></span></li><li><span><a href=\"#Import-of-the-Metadata-File\" data-toc-modified-id=\"Import-of-the-Metadata-File-3\">Import of the Metadata File</a></span></li><li><span><a href=\"#Creation-of-the-Dataset-Class\" data-toc-modified-id=\"Creation-of-the-Dataset-Class-4\">Creation of the Dataset Class</a></span></li><li><span><a href=\"#Instantiation-of-the-Dataset\" data-toc-modified-id=\"Instantiation-of-the-Dataset-5\">Instantiation of the Dataset</a></span></li><li><span><a href=\"#Dataset-Exploration\" data-toc-modified-id=\"Dataset-Exploration-6\">Dataset Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classes-Counts\" data-toc-modified-id=\"Classes-Counts-6.1\">Classes Counts</a></span></li><li><span><a href=\"#Duration-of-Events\" data-toc-modified-id=\"Duration-of-Events-6.2\">Duration of Events</a></span></li><li><span><a href=\"#Salience\" data-toc-modified-id=\"Salience-6.3\">Salience</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global\" data-toc-modified-id=\"Global-6.3.1\">Global</a></span></li><li><span><a href=\"#Per-Class\" data-toc-modified-id=\"Per-Class-6.3.2\">Per Class</a></span></li></ul></li><li><span><a href=\"#Folds-Distribution\" data-toc-modified-id=\"Folds-Distribution-6.4\">Folds Distribution</a></span></li><li><span><a href=\"#Visualition-of-an-Event\" data-toc-modified-id=\"Visualition-of-an-Event-6.5\">Visualition of an Event</a></span></li></ul></li><li><span><a href=\"#Creation-of-the-Model\" data-toc-modified-id=\"Creation-of-the-Model-7\">Creation of the Model</a></span></li><li><span><a href=\"#Creation-of-the-Pipeline\" data-toc-modified-id=\"Creation-of-the-Pipeline-8\">Creation of the Pipeline</a></span></li><li><span><a href=\"#Model-Training-and-Validation\" data-toc-modified-id=\"Model-Training-and-Validation-9\">Model Training and Validation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa371e8",
   "metadata": {},
   "source": [
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1, ConfusionMatrix\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606cb5e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16038d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "dataset_path = \"dataset\"\n",
    "########################################################################\n",
    "# Choose the device to operate on\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Used device: {device}\")\n",
    "########################################################################\n",
    "# Transforms parameters\n",
    "target_sample_rate = 22050\n",
    "target_event_length = 4\n",
    "n_samples = target_event_length * target_sample_rate\n",
    "n_fft = 1024\n",
    "hop_length = 512\n",
    "f_max = 8000\n",
    "n_mels = 64\n",
    "n_frames = (target_sample_rate * target_event_length) // hop_length + 1\n",
    "transforms_params = {\n",
    "    \"target_sample_rate\": target_sample_rate,\n",
    "    \"target_event_length\": target_event_length,\n",
    "    \"n_samples\": n_samples,\n",
    "    \"n_fft\": n_fft,\n",
    "    \"hop_length\": hop_length,\n",
    "    \"f_max\": f_max,\n",
    "    \"n_mels\": n_mels,\n",
    "    \"n_frames\": n_frames,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf857b2",
   "metadata": {},
   "source": [
    "## Import of the Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67767e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"dataset/UrbanSound8K.csv\")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e19056",
   "metadata": {},
   "source": [
    "## Creation of the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSound8K(Dataset):\n",
    "    \n",
    "    def __init__(self, metadata, dataset_path, transforms_params, device):\n",
    "        self.device = device\n",
    "        self.metadata = metadata\n",
    "        self.dataset_path = dataset_path\n",
    "        self.n_folds = max(metadata[\"fold\"])\n",
    "        self.n_classes = len(metadata[\"class\"].unique())\n",
    "        self.classes_map = classes_map = pd.Series(metadata[\"class\"].values,index=metadata[\"classID\"]).sort_index().to_dict()\n",
    "        self.target_sample_rate = transforms_params[\"target_sample_rate\"]\n",
    "        self.target_event_length = transforms_params[\"target_event_length\"]\n",
    "        self.n_samples = transforms_params[\"n_samples\"]\n",
    "        self.n_fft = transforms_params[\"n_fft\"]\n",
    "        self.hop_length = transforms_params[\"hop_length\"]\n",
    "        self.f_max = transforms_params[\"f_max\"]\n",
    "        self.n_mels = transforms_params[\"n_mels\"]\n",
    "        self.n_frames = transforms_params[\"n_frames\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = torch.tensor(self._get_event_class(index), dtype=torch.long)\n",
    "        signal, sr = self._get_event_signal(index)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self._spectrogram_transform(signal)\n",
    "        signal = self._db_transform(signal)\n",
    "        return signal, label\n",
    "    \n",
    "    def _get_event_class(self, index):\n",
    "        return self.metadata.iloc[index][\"classID\"]\n",
    "    \n",
    "    def _get_event_signal(self, index):\n",
    "        event_fold = f\"fold{self.metadata.iloc[index]['fold']}\"\n",
    "        event_filename = self.metadata.iloc[index][\"slice_file_name\"]\n",
    "        audio_path = os.path.join(self.dataset_path, event_fold, event_filename)\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        return signal, sr\n",
    "    \n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        # If signal has multiple channels, mix down to mono\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "        \n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resample_transform = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            resample_transform = resample_transform.to(self.device)\n",
    "            signal = resample_transform(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.n_samples:\n",
    "            signal = signal[:, :self.n_samples]\n",
    "        return signal\n",
    "        \n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        signal_length = signal.shape[1]\n",
    "        if signal_length < self.n_samples:\n",
    "            num_missing_samples = self.n_samples - signal_length\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "    \n",
    "    def _spectrogram_transform(self, signal):\n",
    "        mel_spectrogram_transform = transforms.MelSpectrogram(\n",
    "                                                        sample_rate = self.target_sample_rate,\n",
    "                                                        n_fft = self.n_fft,\n",
    "                                                        hop_length = self.hop_length,\n",
    "                                                        f_max = self.f_max,\n",
    "                                                        n_mels = self.n_mels,\n",
    "                                                        power = 2,\n",
    "                                                        normalized=True\n",
    "                                                        )      \n",
    "        mel_spectrogram_transform = mel_spectrogram_transform.to(self.device)\n",
    "        signal = mel_spectrogram_transform(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _db_transform(self, signal):\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\")\n",
    "        db_transform = db_transform.to(self.device)\n",
    "        signal = db_transform(signal)\n",
    "        return signal\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78432297",
   "metadata": {},
   "source": [
    "## Instantiation of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fcc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a dataset object\n",
    "dataset = UrbanSound8K(\n",
    "    metadata=metadata,\n",
    "    dataset_path=dataset_path, \n",
    "    transforms_params=transforms_params,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2146342",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6be512",
   "metadata": {},
   "source": [
    "### Classes Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vc = dataset.metadata[\"class\"].value_counts()\n",
    "plt.figure(figsize=(18,8))\n",
    "sns.barplot(x=class_vc.index, y=class_vc.values)\n",
    "plt.title(\"Classes Counts\", fontsize=20)\n",
    "plt.xlabel(\"Classes\", fontsize=14)\n",
    "plt.ylabel(\"Counts\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14aee5b",
   "metadata": {},
   "source": [
    "### Duration of Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = dataset.metadata[\"end\"] - dataset.metadata[\"start\"]\n",
    "plt.figure(figsize=(18,8))\n",
    "sns.histplot(data=duration, x=duration.values, bins=20)\n",
    "plt.title(\"Duration of Events\", fontsize=20)\n",
    "plt.xlabel(\"Duration\", fontsize=14)\n",
    "plt.ylabel(\"Counts\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ce867",
   "metadata": {},
   "source": [
    "### Salience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903becf",
   "metadata": {},
   "source": [
    "#### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salience_vc = dataset.metadata[\"salience\"].value_counts()\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(x=salience_vc.index, y=salience_vc.values)\n",
    "plt.title(\"Salience Counts\", fontsize=20)\n",
    "plt.xlabel(\"Salience\", fontsize=14)\n",
    "plt.ylabel(\"Counts\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e66b1",
   "metadata": {},
   "source": [
    "#### Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e49cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set subplot figure parameters\n",
    "n_cols = 5\n",
    "n_rows = math.ceil(len(metadata[\"class\"].unique())/n_cols) # Compute the number of rows based on the number of labels and columns\n",
    "p_count = 1\n",
    "\n",
    "fig = plt.figure(figsize=(30,15))\n",
    "\n",
    "# For each unique class\n",
    "for unique_class in metadata[\"class\"].unique():\n",
    "    # Filter the metadata based on the label\n",
    "    filtered_metadata = metadata[metadata[\"class\"]==unique_class]\n",
    "    # Initialize a dictionnary to count the number events for each salience score\n",
    "    salience_dict = {1:0, 2:0}\n",
    "    # For each row of the filtered metadata\n",
    "    for index, row in filtered_metadata.iterrows():\n",
    "        # Get the salience score\n",
    "        salience_score = row[4]\n",
    "        # Populate the dictionnary\n",
    "        if salience_score not in salience_dict:\n",
    "            salience_dict[salience_score] = 1\n",
    "        else:\n",
    "            salience_dict[salience_score] += 1\n",
    "    plt.subplot(n_rows, n_cols, p_count) \n",
    "    # Plot a bar graph with all salience scores on the x axis\n",
    "    plt.bar(range(len(salience_dict)), list(salience_dict.values()), align='center')\n",
    "    plt.ylim([0, 1000])\n",
    "    # Write the counts on each bar\n",
    "    for salience in salience_dict.keys():\n",
    "        plt.text(salience-1, (filtered_metadata['salience'] == salience).sum()+10, str((filtered_metadata['salience'] == salience).sum()), fontsize=14)\n",
    "    # Write the ticks on the x axis\n",
    "    plt.xticks(range(len(salience_dict)), list(salience_dict.keys()), fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.title(f\"{unique_class}\", fontweight=\"bold\", fontsize=18)\n",
    "    plt.xlabel(\"Score de Confiance de LabÃ©lisation\", fontweight=\"bold\", fontsize=14)\n",
    "    plt.ylabel(\"Nombre d'observations\", fontweight=\"bold\", fontsize=14)\n",
    "    p_count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943f0b02",
   "metadata": {},
   "source": [
    "### Folds Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409eb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by fold and label and pivot\n",
    "folds_vc = metadata.groupby([\"fold\", \"class\"], dropna=False, as_index=False)[\"class\"].size()\n",
    "folds_vc.pivot(index=\"class\", columns=\"fold\", values=\"size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d556b17",
   "metadata": {},
   "source": [
    "### Visualition of an Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c241352",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Input annotations filtering critera\n",
    "label = \"gun_shot\"\n",
    "salience = 1 \n",
    "\n",
    "# Filter annotations based on criteria\n",
    "filtered_metadata = dataset.metadata.loc[\n",
    "    (metadata[\"class\"]==label)\n",
    "    & (metadata[\"salience\"]==salience)\n",
    "    ]\n",
    "\n",
    "# Randomly select the desired number of events from the filtered annotations\n",
    "selected_event = filtered_metadata.sample(n=1) \n",
    "display(selected_event)\n",
    "selected_event_index = selected_event.index.item()\n",
    "\n",
    "spectrogram, label = dataset[selected_event_index]\n",
    "\n",
    "spectrogram = spectrogram.cpu()\n",
    "spectrogram = torch.squeeze(spectrogram)\n",
    "\n",
    "print(spectrogram)\n",
    "print(spectrogram.max())\n",
    "print(spectrogram.min())\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(spectrogram, origin=\"lower\", cmap='plasma')\n",
    "\n",
    "# Build the audio path\n",
    "selected_event_fold = f\"fold{selected_event.iloc[0]['fold']}\"\n",
    "selected_event_filename = selected_event.iloc[0][\"slice_file_name\"]\n",
    "selected_audio_path = os.path.join(dataset_path, selected_event_fold, selected_event_filename)\n",
    "display(IPython.display.Audio(filename=selected_audio_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f141bed",
   "metadata": {},
   "source": [
    "## Creation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        # Definition of the model\n",
    "        self.conv1 = nn.Sequential(\n",
    "                                nn.Conv2d(\n",
    "                                            in_channels=1,\n",
    "                                            out_channels=16,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=2\n",
    "                                            ),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=2)\n",
    "                                )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                                nn.Conv2d(\n",
    "                                            in_channels=16,\n",
    "                                            out_channels=32,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=2\n",
    "                                            ),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=2)\n",
    "                                )\n",
    "        self.conv3 = nn.Sequential(\n",
    "                                nn.Conv2d(\n",
    "                                            in_channels=32,\n",
    "                                            out_channels=64,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=2\n",
    "                                            ),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=2)\n",
    "                                )\n",
    "        self.conv4 = nn.Sequential(\n",
    "                                nn.Conv2d(\n",
    "                                            in_channels=64,\n",
    "                                            out_channels=128,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=2\n",
    "                                            ),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=2)\n",
    "                                )  \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 5 * 12, out_dim)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c784a5",
   "metadata": {},
   "source": [
    "## Creation of the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(pl.LightningModule):\n",
    "    def __init__(self, model, classes_map, lr):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save hyperparameters to the checkpoint\n",
    "        self.save_hyperparameters()        \n",
    "\n",
    "        # Instantiate the model\n",
    "        self.model = model\n",
    "           \n",
    "        # Instiation of the metrics\n",
    "        self.accuracy = Accuracy(num_classes=len(classes_map), average=\"weighted\")\n",
    "        self.recall = Recall(num_classes=len(classes_map), average=\"weighted\")\n",
    "        self.f1_score = F1(num_classes=len(classes_map), average=\"weighted\")\n",
    "        self.confmat = ConfusionMatrix(num_classes=len(classes_map))           \n",
    "        \n",
    "        # Instantiation of the classes map\n",
    "        self.classes_map = classes_map\n",
    "        \n",
    "        # Instantiation of the number of classes\n",
    "        self.n_classes = len(classes_map)\n",
    "        \n",
    "        # Instatiation of the learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "            \n",
    "        return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"monitor\": \"validation_loss\",\n",
    "                        \"frequency\": 1\n",
    "                        }\n",
    "                }\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx): \n",
    "        \n",
    "        # Unpack the training batch\n",
    "        inputs, targets = train_batch\n",
    "        # Pass the inputs to the model to get the logits\n",
    "        logits = self.model(inputs)\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        # Get the probabilities for each class by applying softmax\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Get the prediction for each batch sample\n",
    "        _, preds = torch.max(probs, 1)\n",
    "        # Compute the accuracy\n",
    "        accuracy = self.accuracy(logits, targets)\n",
    "        # Log the loss\n",
    "        self.log(\"training_loss\", loss, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return {\"inputs\":inputs, \"targets\":targets, \"predictions\":preds, \"loss\":loss}\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        \n",
    "        if self.current_epoch == 0:\n",
    "            input_height = outputs[0][\"inputs\"].size()[2]\n",
    "            input_width = outputs[0][\"inputs\"].size()[3]\n",
    "            input_sample = torch.rand((1,1,input_height,input_width)).to(self.device)\n",
    "            self.logger.experiment.add_graph(self.model, input_sample)\n",
    "\n",
    "            \n",
    "    def validation_step(self, validation_batch, batch_idx):\n",
    "        \n",
    "        # Unpack the validation batch\n",
    "        inputs, targets = validation_batch\n",
    "        # Pass the inputs to the model to get the logits\n",
    "        logits = self.model(inputs)\n",
    "        # Compute the loss and log it for early stopping monitoring\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        # Get the probabilities for each class by applying softmax\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Get the prediction for each batch sample\n",
    "        _, preds = torch.max(probs, 1)\n",
    "        # Compute the accuracy for this batch\n",
    "        accuracy = self.accuracy(preds, targets)\n",
    "        # Log the loss and the accuracy\n",
    "        self.log(\"validation_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return {\"inputs\":inputs, \"targets\":targets, \"predictions\":preds, \"loss\":loss}\n",
    "    \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \n",
    "        # Concatenate the predictions of all batches\n",
    "        preds = torch.cat([output[\"predictions\"] for output in outputs])\n",
    "        # Concatenate the targets of all batches\n",
    "        targets = torch.cat([output[\"targets\"] for output in outputs])\n",
    "        # Compute the confusion matrix, turn it into a DataFrame, generate the plot and log it\n",
    "        cm = self.confmat(preds, targets)\n",
    "        cm = cm.cpu()\n",
    "        \n",
    "        for class_id in range(self.n_classes):\n",
    "                precision = cm[class_id, class_id] / torch.sum(cm[:,class_id])\n",
    "                precision = round(precision.item()*100,1)\n",
    "                self.log(f\"validation_precision/{class_id}\",precision)\n",
    "                recall = cm[class_id, class_id] / torch.sum(cm[class_id,:])\n",
    "                recall = round(recall.item()*100,1)\n",
    "                self.log(f\"validation_recall/{class_id}\",recall)\n",
    "      \n",
    "        df_cm = pd.DataFrame(cm.numpy(), index = range(self.n_classes), columns=range(self.n_classes))\n",
    "        plt.figure()\n",
    "        fig = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "        plt.yticks(rotation=0)\n",
    "        self.logger.experiment.add_figure(\"Confusion matrix\", fig, self.current_epoch)\n",
    "        \n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        # Get the state_dict from self.model to get rid of the \"model.\" prefix\n",
    "        checkpoint[\"state_dict\"] = self.model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cc51b",
   "metadata": {},
   "source": [
    "## Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06102b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 32\n",
    "# Number of epochs\n",
    "n_epochs = 30\n",
    "# Learning rate\n",
    "learning_rate = 2e-4\n",
    "\n",
    "num_classes = len(dataset.metadata[\"class\"].unique())\n",
    "\n",
    "for i in range(1,dataset.n_folds+1):\n",
    "    \n",
    "    print(f\"========== Cross-validation {i} on {dataset.n_folds} ==========\")\n",
    "    \n",
    "    # Get the train and validation sets\n",
    "    train_metadata = dataset.metadata.drop(dataset.metadata[dataset.metadata[\"fold\"]==i].index)\n",
    "    validation_metadata = dataset.metadata[dataset.metadata[\"fold\"]==i]\n",
    "    train_indices = train_metadata.index\n",
    "    validation_indices = validation_metadata.index \n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "    \n",
    "    # Create the train and validation dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "                            dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            sampler=train_sampler,\n",
    "                            )\n",
    "    \n",
    "    validation_dataloader = DataLoader(\n",
    "                            dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            sampler=validation_sampler,\n",
    "                            )\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = Model(out_dim=dataset.n_classes)\n",
    "    \n",
    "    # Instantiate the pipeline\n",
    "    pipeline = Pipeline(model, classes_map=dataset.classes_map, lr=learning_rate)\n",
    "    \n",
    "    # Instantiate the logger\n",
    "    run_name = f\"{dataset.n_folds} folds cross-validation - Validation on fold {i}\"\n",
    "    tensorboard_logger = TensorBoardLogger(save_dir=\"logs\", name=run_name)\n",
    "    \n",
    "    # Instantiate a learning rate monitor\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    \n",
    "    # Instantiate early stopping based on epoch validation loss\n",
    "    early_stopping = EarlyStopping(\"validation_loss\", patience=6, verbose=True)\n",
    "    \n",
    "    # Instantiate a checkpoint callback\n",
    "    checkpoint = ModelCheckpoint(\n",
    "                            dirpath=f\"./checkpoints/{dataset.n_folds} folds cross-validation - Validation on fold {i}\",\n",
    "                            filename=\"{epoch}-{validation_loss:.2f}\",\n",
    "                            verbose=True,\n",
    "                            monitor=\"validation_loss\",\n",
    "                            save_last = False,\n",
    "                            save_top_k=1,      \n",
    "                            mode=\"min\",\n",
    "                            save_weights_only=True\n",
    "                            )\n",
    "    \n",
    "    # Instantiate the trainer and train the model\n",
    "    trainer = Trainer(\n",
    "                    gpus=-1,\n",
    "                    max_epochs=n_epochs, \n",
    "                    logger=tensorboard_logger,\n",
    "                    log_every_n_steps = 1,\n",
    "                    callbacks=[early_stopping, lr_monitor, checkpoint]\n",
    "                    )   \n",
    "    \n",
    "    trainer.fit(pipeline, train_dataloader, validation_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
