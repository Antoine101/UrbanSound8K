{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import IPython\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89814a62",
   "metadata": {},
   "source": [
    "## Import of the Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"dataset/UrbanSound8K.csv\")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55a5bc",
   "metadata": {},
   "source": [
    "## Audio Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ccd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = 4\n",
    "target_sample_rate = 22050\n",
    "n_samples = target_length * target_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ffc1c",
   "metadata": {},
   "source": [
    "## Selection of the Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075453f",
   "metadata": {},
   "source": [
    "For a recall, the different classes present in the UrbanSound8K dataset are:\n",
    "- air_conditioner\n",
    "- car_horn\n",
    "- children_playing\n",
    "- dog_bark\n",
    "- drilling\n",
    "- engine_idling\n",
    "- gun_shot\n",
    "- jackhammer\n",
    "- siren\n",
    "- street_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input annotations filtering critera\n",
    "label = \"street_music\"\n",
    "salience = 1 \n",
    "\n",
    "# Filter annotations based on criteria\n",
    "filtered_metadata = metadata.loc[\n",
    "    (metadata[\"class\"]==label)\n",
    "    & (metadata[\"salience\"]==salience)\n",
    "    ]\n",
    "\n",
    "# Randomly select an audio from the filtered annotations\n",
    "selected_audio = filtered_metadata.sample(n=1) \n",
    "display(selected_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd09269",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in selected_audio.iterrows():\n",
    "    audio_filename = row[\"slice_file_name\"]\n",
    "    audio_fold = f\"fold{row['fold']}\"\n",
    "    audio_path = os.path.join(\"dataset\", audio_fold, audio_filename)\n",
    "    # Load the audio signal\n",
    "    audio_signal, sr = torchaudio.load(audio_path)\n",
    "    # Mix it down to mono if necessary\n",
    "    if audio_signal.shape[0] > 1:\n",
    "        audio_signal = torch.mean(audio_signal, dim=0, keepdim=True)\n",
    "    # Resample it\n",
    "    resample_transform = transforms.Resample(sr, target_sample_rate)\n",
    "    audio_signal = resample_transform(audio_signal)\n",
    "    # Cut if necessary\n",
    "    if audio_signal.shape[1] > n_samples:\n",
    "        audio_signal = audio_signal[:, :n_samples]\n",
    "    # Right pad if necessary\n",
    "    if audio_signal.shape[1] < n_samples:\n",
    "        n_missing_samples = n_samples - audio_signal.shape[1]\n",
    "        last_dim_padding = (0, n_missing_samples)\n",
    "        audio_signal = nn.functional.pad(audio_signal, last_dim_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4930abd6",
   "metadata": {},
   "source": [
    "## Spectrogram Parameters Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1aba86",
   "metadata": {},
   "source": [
    "### FFT Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42505653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_fft_values = [128, 256, 512, 1024]\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(len(n_fft_values)/n_cols)\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "amplitude_to_db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "                    # Scale of input tensor (\"power\" or \"magnitude\"). The power being the elementwise square of the magnitude. (Default: \"power\")\n",
    "                    stype = \"power\",\n",
    "                    # Minimum negative cut-off in decibels. A reasonable number is 80. (Default: None)\n",
    "                    top_db = None\n",
    "                    )\n",
    "\n",
    "for i, n_fft_value in enumerate(n_fft_values):\n",
    "    \n",
    "        spectrogram_transform = torchaudio.transforms.Spectrogram( \n",
    "                    # Size of FFT, creates n_fft // 2 + 1 bins. (Default: 400)\n",
    "                    n_fft = n_fft_value,\n",
    "                    # Window size. (Default: n_fft)\n",
    "                    win_length = n_fft_value,\n",
    "                    # Length of hop between STFT windows. (Default: win_length // 2)\n",
    "                    hop_length = n_fft_value // 2,\n",
    "                    # Two sided padding of signal. (Default: 0)\n",
    "                    pad = 0,\n",
    "                    # A function to create a window tensor that is applied/multiplied to each frame/window. (Default: torch.hann_window)\n",
    "                    window_fn = torch.hann_window,\n",
    "                    # Exponent for the magnitude spectrogram, (must be > 0) e.g., 1 for energy, 2 for power, etc. (Default: 2)\n",
    "                    power = 2,\n",
    "                    # Whether to normalize by magnitude after stft. (Default: False)\n",
    "                    normalized = True,\n",
    "                    # Arguments for window function. (Default: None)\n",
    "                    wkwargs = None,\n",
    "                    # Whether to pad waveform on both sides so that the t-th frame is centered at time t x hop_length (Default: True)\n",
    "                    center = False,\n",
    "                    # Controls the padding method used when center is True. (Default: \"reflect\")\n",
    "                    pad_mode = \"reflect\",\n",
    "                    # Controls whether to return half of results to avoid redundancy. (Default: True)\n",
    "                    onesided = True,\n",
    "                    # Indicates whether the resulting complex-valued Tensor should be represented with native complex dtype, \n",
    "                    # such as torch.cfloat and torch.cdouble, or real dtype mimicking complex value with an extra dimension \n",
    "                    # for real and imaginary parts. (See also torch.view_as_real.)\n",
    "                    # This argument is only effective when power=None. It is ignored for cases where power is a number as in those cases, the returned tensor is power spectrogram, which is a real-valued tensor.\n",
    "                    return_complex = False\n",
    "                    )   \n",
    "        \n",
    "        num_channels, num_frames = audio_signal.shape\n",
    "        time_axis = torch.arange(0, num_frames) / target_sample_rate\n",
    "    \n",
    "        spectrogram = torch.squeeze(spectrogram_transform(audio_signal)[0])\n",
    "        spectrogram_db = amplitude_to_db_transform(spectrogram)\n",
    "    \n",
    "        n_fft_spec = (spectrogram_db.shape[0] - 1) * 2\n",
    "        frequency = (target_sample_rate / n_fft_spec) * np.linspace(0, n_fft_spec/2, spectrogram_db.shape[0])\n",
    "        max_frequency_bin = frequency.max()\n",
    "    \n",
    "        ax = fig.add_subplot(2, 2, i+1)\n",
    "        ax.imshow(spectrogram_db, extent=[0, target_length, 0, target_sample_rate/2], origin=\"lower\", aspect=\"auto\")\n",
    "        ax.set_title(f\"n_fft : {n_fft_value}\", fontweight=\"bold\", fontsize=12)\n",
    "        ax.set_ylabel(\"Frequency (Hz)\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e156f10",
   "metadata": {},
   "source": [
    "### Hop Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length_divisors = [8, 4, 2, 1]\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(len(n_fft_values)/n_cols)\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "amplitude_to_db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "                    # Scale of input tensor (\"power\" or \"magnitude\"). The power being the elementwise square of the magnitude. (Default: \"power\")\n",
    "                    stype = \"power\",\n",
    "                    # Minimum negative cut-off in decibels. A reasonable number is 80. (Default: None)\n",
    "                    top_db = None\n",
    "                    )\n",
    "\n",
    "for i, hop_length_divisor in enumerate(hop_length_divisors):\n",
    "    \n",
    "        spectrogram_transform = torchaudio.transforms.Spectrogram( \n",
    "                    # Size of FFT, creates n_fft // 2 + 1 bins. (Default: 400)\n",
    "                    n_fft = 1024,\n",
    "                    # Window size. (Default: n_fft)\n",
    "                    win_length = 1024,\n",
    "                    # Length of hop between STFT windows. (Default: win_length // 2)\n",
    "                    hop_length = 1024 // hop_length_divisor,\n",
    "                    # Two sided padding of signal. (Default: 0)\n",
    "                    pad = 0,\n",
    "                    # A function to create a window tensor that is applied/multiplied to each frame/window. (Default: torch.hann_window)\n",
    "                    window_fn = torch.hann_window,\n",
    "                    # Exponent for the magnitude spectrogram, (must be > 0) e.g., 1 for energy, 2 for power, etc. (Default: 2)\n",
    "                    power = 2,\n",
    "                    # Whether to normalize by magnitude after stft. (Default: False)\n",
    "                    normalized = True,\n",
    "                    # Arguments for window function. (Default: None)\n",
    "                    wkwargs = None,\n",
    "                    # Whether to pad waveform on both sides so that the t-th frame is centered at time t x hop_length (Default: True)\n",
    "                    center = False,\n",
    "                    # Controls the padding method used when center is True. (Default: \"reflect\")\n",
    "                    pad_mode = \"reflect\",\n",
    "                    # Controls whether to return half of results to avoid redundancy. (Default: True)\n",
    "                    onesided = True,\n",
    "                    # Indicates whether the resulting complex-valued Tensor should be represented with native complex dtype, \n",
    "                    # such as torch.cfloat and torch.cdouble, or real dtype mimicking complex value with an extra dimension \n",
    "                    # for real and imaginary parts. (See also torch.view_as_real.)\n",
    "                    # This argument is only effective when power=None. It is ignored for cases where power is a number as in those cases, the returned tensor is power spectrogram, which is a real-valued tensor.\n",
    "                    return_complex = False\n",
    "                    )   \n",
    "        \n",
    "        num_channels, num_frames = audio_signal.shape\n",
    "        time_axis = torch.arange(0, num_frames) / target_sample_rate\n",
    "    \n",
    "        spectrogram = torch.squeeze(spectrogram_transform(audio_signal)[0])\n",
    "        spectrogram_db = amplitude_to_db_transform(spectrogram)\n",
    "    \n",
    "        n_fft_spec = (spectrogram_db.shape[0] - 1) * 2\n",
    "        frequency = (target_sample_rate / n_fft_spec) * np.linspace(0, n_fft_spec/2, spectrogram_db.shape[0])\n",
    "        max_frequency_bin = frequency.max()\n",
    "    \n",
    "        ax = fig.add_subplot(2, 2, i+1)\n",
    "        ax.imshow(spectrogram_db, extent=[0, target_length, 0, target_sample_rate/2], origin=\"lower\", aspect=\"auto\")\n",
    "        ax.set_title(f\"hop_length : {1024 // hop_length_divisor}\", fontweight=\"bold\", fontsize=12)\n",
    "        ax.set_ylabel(\"Frequency (Hz)\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
