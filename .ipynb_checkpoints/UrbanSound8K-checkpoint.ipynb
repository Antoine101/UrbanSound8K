{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b7d626",
   "metadata": {},
   "source": [
    "# <center>UrbanSound8K</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53752567",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-of-Required-Libraries\" data-toc-modified-id=\"Import-of-Required-Libraries-1\">Import of Required Libraries</a></span></li><li><span><a href=\"#Creation-the-Dataset-Class\" data-toc-modified-id=\"Creation-the-Dataset-Class-2\">Creation the Dataset Class</a></span></li><li><span><a href=\"#Creation-of-the-Transforms-for-Audio-Pre-Processing\" data-toc-modified-id=\"Creation-of-the-Transforms-for-Audio-Pre-Processing-3\">Creation of the Transforms for Audio Pre-Processing</a></span></li><li><span><a href=\"#Creation-of-the-Model\" data-toc-modified-id=\"Creation-of-the-Model-4\">Creation of the Model</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5\">Test</a></span></li><li><span><a href=\"#Dataset-Exploration\" data-toc-modified-id=\"Dataset-Exploration-6\">Dataset Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classes-Counts\" data-toc-modified-id=\"Classes-Counts-6.1\">Classes Counts</a></span></li><li><span><a href=\"#Duration-of-Events\" data-toc-modified-id=\"Duration-of-Events-6.2\">Duration of Events</a></span></li><li><span><a href=\"#Analysis-of-Salience\" data-toc-modified-id=\"Analysis-of-Salience-6.3\">Analysis of Salience</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global\" data-toc-modified-id=\"Global-6.3.1\">Global</a></span></li><li><span><a href=\"#Per-Class\" data-toc-modified-id=\"Per-Class-6.3.2\">Per Class</a></span></li></ul></li><li><span><a href=\"#Folds-Distribution\" data-toc-modified-id=\"Folds-Distribution-6.4\">Folds Distribution</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa371e8",
   "metadata": {},
   "source": [
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bec3743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e19056",
   "metadata": {},
   "source": [
    "## Creation of the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d26ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSound8K(Dataset):\n",
    "    \n",
    "    def __init__(self, annotations, dataset_path, transforms_params, device):\n",
    "        self.device = device\n",
    "        self.annotations = annotations\n",
    "        self.dataset_path = dataset_path\n",
    "        self.target_sample_rate = transforms_params[\"target_sample_rate\"]\n",
    "        self.target_event_length = transforms_params[\"target_event_length\"]\n",
    "        self.num_samples = target_event_length * target_sample_rate\n",
    "        self.n_fft = transforms_params[\"n_fft\"]\n",
    "        self.hop_length = transforms_params[\"hop_length\"]\n",
    "        self.f_max = transforms_params[\"f_max\"]\n",
    "        self.n_mels = transforms_params[\"n_mels\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = torch.tensor(self._get_event_label(index), dtype=torch.long)\n",
    "        signal, sr = self._get_event_signal(index)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self._spectrogram_transform(signal)\n",
    "        signal = self._db_transform(signal)\n",
    "        return signal, label\n",
    "    \n",
    "    def _get_event_label(self, index):\n",
    "        return self.annotations.iloc[index][\"labelID\"]\n",
    "    \n",
    "    def _get_event_signal(self, index):\n",
    "        audio_path = os.path.join(self.dataset_path, self.annotations.iloc[index][\"audio\"])\n",
    "        audio_signal, sr = torchaudio.load(audio_path)\n",
    "        start_index = math.floor(self.annotations.iloc[index][\"start\"] * sr) - 1\n",
    "        if start_index < 0:\n",
    "            start_index = 0\n",
    "        end_index = math.ceil(self.annotations.iloc[index][\"end\"] * sr) - 1\n",
    "        event_signal = audio_signal[:, start_index:end_index]\n",
    "        return event_signal, sr\n",
    "    \n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        # If signal has multiple channels, mix down to mono\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "        \n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resample_transform = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            resample_transform = resample_transform.to(self.device)\n",
    "            signal = resample_transform(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "        \n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "    \n",
    "    def _spectrogram_transform(self, signal):\n",
    "        mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
    "                                                        sample_rate = self.target_sample_rate,\n",
    "                                                        n_fft = self.n_fft,\n",
    "                                                        hop_length = self.hop_length,\n",
    "                                                        f_max = self.f_max,\n",
    "                                                        n_mels = self.n_mels,\n",
    "                                                        power = 2\n",
    "                                                        )      \n",
    "        mel_spectrogram_transform = mel_spectrogram_transform.to(self.device)\n",
    "        signal = mel_spectrogram_transform(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _db_transform(self, signal):\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype='power')\n",
    "        db_transform = db_transform.to(self.device)\n",
    "        signal = db_transform(signal)\n",
    "        return signal\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78432297",
   "metadata": {},
   "source": [
    "## Instantiation of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fcc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a dataset object\n",
    "dataset = UrbanSound8K(\n",
    "    annotations=annotations, \n",
    "    dataset_path=dataset_path, \n",
    "    transforms_params=transforms_params,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2146342",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6be512",
   "metadata": {},
   "source": [
    "### Classes Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vc = dataset.annotations[\"class\"].value_counts()\n",
    "plt.figure(figsize=(18,8))\n",
    "sns.barplot(x=class_vc.index, y=class_vc.values)\n",
    "plt.title(\"Classes Counts\", fontsize=20)\n",
    "plt.xlabel(\"Classes\", fontsize=14)\n",
    "plt.ylabel(\"Counts\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14aee5b",
   "metadata": {},
   "source": [
    "### Duration of Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = dataset.annotations[\"end\"] - dataset.annotations[\"start\"]\n",
    "plt.figure(figsize=(18,8))\n",
    "sns.histplot(data=duration, x=duration.values, bins=20)\n",
    "plt.title(\"Duration of Events\", fontsize=20)\n",
    "plt.xlabel(\"Duration\", fontsize=14)\n",
    "plt.ylabel(\"Counts\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ce867",
   "metadata": {},
   "source": [
    "### Salience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903becf",
   "metadata": {},
   "source": [
    "#### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salience_vc = dataset.annotations[\"salience\"].value_counts()\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(x=salience_vc.index, y=salience_vc.values)\n",
    "plt.title(\"Salience Counts\", fontsize=20)\n",
    "plt.xlabel(\"Salience\", fontsize=14)\n",
    "plt.ylabel(\"Counts\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e66b1",
   "metadata": {},
   "source": [
    "#### Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e49cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "943f0b02",
   "metadata": {},
   "source": [
    "### Folds Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f141bed",
   "metadata": {},
   "source": [
    "## Creation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of classes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 5 * 12, self.num_classes)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90ce1e",
   "metadata": {},
   "source": [
    "## Creation of the Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691764f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, loss_fn, optimiser, num_epochs, step_size, device):\n",
    "    \n",
    "    # Get the number of batches\n",
    "    n_batches = len(data_loader)\n",
    "    \n",
    "    # Initialise running variables that will be reset after each step\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    # For each epoch\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # For each batch\n",
    "        for j, (inputs, targets) in enumerate(data_loader):\n",
    "        \n",
    "            # Send the inputs and targets to the device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        \n",
    "            # Backward and optimize\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            # Get the running loss to write to tensorboard\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            running_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "            if (j+1) % step_size == 0:\n",
    "                print(f\"Epoch {i+1}/{num_epochs}, Batch {j+1}/{n_batches}, Loss: {loss.item():.4f}\")\n",
    "                # Reset running loss and correct for next step\n",
    "                running_loss = 0.0\n",
    "                running_correct = 0\n",
    "    \n",
    "    print(\"Traning is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b442d08",
   "metadata": {},
   "source": [
    "## Creation of the Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b47d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_loader, classes_map):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Get the number of classes\n",
    "        num_classes = len(classes_map)\n",
    "        \n",
    "        # Initialise `all_targets` and `all_predictions` tensors (staked later and used to plot the confusion matrix)\n",
    "        all_targets = torch.tensor([], dtype=torch.int32).to(device)\n",
    "        all_predictions = torch.tensor([], dtype=torch.int32).to(device)\n",
    "        \n",
    "        # Initialise counters for the number of correct predictions and number of samples\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        \n",
    "        # Initialise dictionnaries for the number of correct predictions per class and number of samples per class\n",
    "        n_correct_per_class = {x:0 for x in range(num_classes)}\n",
    "        n_samples_per_class = {x:0 for x in range(num_classes)}\n",
    "        \n",
    "        # Initialise a dictionnary to store prediction accuracy per class\n",
    "        accuracy_per_class = {x:0 for x in classes_map.values()}\n",
    "    \n",
    "        # For each batch in the validation dataloader\n",
    "        for inputs, targets in data_loader:\n",
    "\n",
    "            # Send the inputs and targets to the device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)            \n",
    "            \n",
    "            # Append the batch targets to `all_targets` \n",
    "            all_targets = torch.cat((all_targets, targets), dim=0)\n",
    "        \n",
    "            # Pass the inputs through the model\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            # Get the predictions\n",
    "            _, predictions = torch.max(outputs.data, 1)\n",
    "            all_predictions = torch.cat((all_predictions, predictions), dim=0)\n",
    "        \n",
    "            # Increment the counters\n",
    "            n_samples += inputs.size(0)\n",
    "            n_correct += (predictions == targets).sum().item()\n",
    "        \n",
    "            # For each sample in the batch\n",
    "            for i in range(len(targets)):\n",
    "                # Get its true class and predicted class\n",
    "                target = targets[i]\n",
    "                predicted = predictions[i]\n",
    "                # If the prediction is correct\n",
    "                if (target == predicted):\n",
    "                    # Increment the counter for this class in the `n_correct_per_class` dictionnary\n",
    "                    n_correct_per_class[target.item()] += 1\n",
    "                # Increment the counter for this class in the `n_samples_per_class` dictionnary\n",
    "                n_samples_per_class[target.item()] += 1\n",
    "         \n",
    "        # Stack `all_targets` and `all_predictions` together\n",
    "        tp_stack = torch.stack((all_targets, all_predictions), dim=1)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = torch.zeros(num_classes, num_classes, dtype=torch.int32).to(device)\n",
    "        for tp in tp_stack:\n",
    "            t, p = tp.tolist()\n",
    "            cm[t, p] += 1\n",
    "        \n",
    "        # Calculate the global prediction accuracy\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        acc = round(acc, 2)\n",
    "        print(f\"Accuracy of the model: {acc}%\")\n",
    "    \n",
    "        # For each classs\n",
    "        for i in range(num_classes):\n",
    "            # Calculate its respective prediction accuracy\n",
    "            if n_samples_per_class[i] == 0:\n",
    "                class_acc = 0\n",
    "            else:\n",
    "                class_acc = 100.0 * n_correct_per_class[i] / n_samples_per_class[i]\n",
    "            accuracy_per_class[classes_map[i]] = class_acc\n",
    "            \n",
    "        return acc, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2bfb87",
   "metadata": {},
   "source": [
    "## Creation of the Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fde349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_data)\n",
    "        predicted = predictions[0].argmax(0)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cc51b",
   "metadata": {},
   "source": [
    "## Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionnary that matches each label with its ID\n",
    "classes_map = dict(enumerate(dataset.annotations[\"label\"].cat.categories))\n",
    "\n",
    "# Instantiate the CNN\n",
    "model = ConvNet(num_classes=len(classes_map)).to(device)\n",
    "\n",
    "# Print the model summary (input shapes and parameters for each layer)\n",
    "print(summary(model, torch.zeros((batch_size, 1, n_mels, n_frames)).to(device), show_input=True))\n",
    "\n",
    "# Initialise the loss function and the optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialise a dictionnary to store each folds combination validation metrics\n",
    "metrics = {f\"Validation on fold {x+1}\":{\"acc\":[], \"cm\":[]} for x in range(n_folds)}\n",
    "\n",
    "for i in range(1,n_folds+1):\n",
    "    # Get the train and validation sets\n",
    "    train_annotations = dataset.annotations.drop(dataset.annotations[dataset.annotations[\"fold\"]==i].index)\n",
    "    validation_annotations = dataset.annotations[dataset.annotations[\"fold\"]==i]\n",
    "    train_indices = train_annotations.index\n",
    "    validation_indices = validation_annotations.index\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "    \n",
    "    # Create the train and validation dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "                            dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            sampler=train_sampler\n",
    "                            )\n",
    "    validation_dataloader = DataLoader(\n",
    "                            dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            sampler=validation_sampler\n",
    "                            )\n",
    "    \n",
    "    # Train the model\n",
    "    train(model, train_dataloader, loss_fn, optimiser, num_epochs, step_size, device)\n",
    "\n",
    "    # Validate the model\n",
    "    classes_map = dict(enumerate(annotations[\"label\"].cat.categories))\n",
    "    acc, cm = validate(model, validation_dataloader, classes_map)\n",
    "    metrics[f\"Validation on fold {i}\"][\"acc\"] = acc\n",
    "    metrics[f\"Validation on fold {i}\"][\"cm\"] = cm\n",
    "    \n",
    "\n",
    "    # Save the model\n",
    "    filename = f\"SonoLog-Classifier_validation-Fold{i}_acc-{acc}.pth\"\n",
    "    torch.save(model.state_dict(), os.path.join(\"Models\",filename))\n",
    "    print(f\"Model stored as file : {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d94e27",
   "metadata": {},
   "source": [
    "## Visualisation of the Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes_map, normalize=False):\n",
    "    cm = cm.cpu()\n",
    "    if normalize:\n",
    "        cm = cm/cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix without normalization\")\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\", fontsize=20)\n",
    "    plt.colorbar=()\n",
    "    tick_marks = np.arange(len(classes_map))\n",
    "    plt.xticks(tick_marks, classes_map.values(), rotation=90, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes_map.values(), fontsize=12)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), \n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Predicted Class\", fontsize=16)\n",
    "    plt.ylabel(\"True Class\", fontsize=16)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plot_confusion_matrix(cm, classes_map, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58865929",
   "metadata": {},
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01db9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back (if needed)\n",
    "model = ConvNet(num_classes=len(classes_map))\n",
    "state_dict = torch.load(\"Models/SonoLog-Classifier_validation-Fold5_acc-100.0.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample from the validation set for inference\n",
    "sample_index = 11\n",
    "input_data, target = dataset[sample_index][0], dataset[sample_index][1]\n",
    "input_data = torch.unsqueeze(input_data, dim=0)\n",
    "input_data, target = input_data.to(device), target.to(device)\n",
    "\n",
    "# Make an inference\n",
    "predicted = predict(model, input_data)\n",
    "\n",
    "predicted_class = classes_map[predicted.item()]\n",
    "expected_class = classes_map[target.item()]\n",
    "        \n",
    "print(f\"Predicted: '{predicted_class}', Expected: '{expected_class}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
